#!/bin/bash

echo "ğŸ¢ Setting up Telsec RAG Assistant UI"
echo "=================================="

# Check if virtual environment exists
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Creating virtual environment..."
    python3 -m venv venv
fi

# Activate virtual environment
echo "ğŸ”Œ Activating virtual environment..."
source venv/bin/activate

# Install requirements
echo "ğŸ“¥ Installing dependencies..."
pip install -r requirements.txt

# Check if Ollama is running (for the LLM)
echo "ğŸ¤– Checking Ollama status..."
if ! pgrep -x "ollama" > /dev/null; then
    echo "âš ï¸  Warning: Ollama is not running. Please start it with: ollama serve"
    echo "ğŸ’¡ Also ensure you have the llama3.2 model: ollama pull llama3.2"
fi

echo ""
echo "âœ… Setup complete!"
echo "ğŸš€ Run the app with: python launch.py"
echo "ğŸŒ Or directly with: streamlit run app.py"
echo ""
echo "ğŸ¨ Your UI will be available at: http://localhost:8501" 